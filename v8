# -*- coding: utf-8 -*-
"""Detta8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a7DyemL4c9OACsPih09eE-0xhTMEZ2hd
"""

!pip install transformers[torch] fastapi uvicorn pydantic nest-asyncio torch==2.2.1 google-colab accelerate>={ACCELERATE_MIN_VERSION} -q

import accelerate
from accelerate import Accelerator
accelerator = Accelerator()
import os
import torch
from google.colab import drive
from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments
from torch.utils.data import Dataset

drive.mount('/content/drive', force_remount=True)

def list_files_recursively(base_folder):
    """Recursively list all files in the base_folder and subfolders."""
    file_paths = []
    for root, dirs, files in os.walk(base_folder):
        for file in files:
            file_paths.append(os.path.join(root, file))
    return file_paths

class CodeDataset(Dataset):
    def __init__(self, tokenizer, file_paths, block_size=512):
        self.input_ids = []
        self.attention_masks = []
        for file_path in file_paths:
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
            if not text.strip():
                print(f"Skipping empty or whitespace-only file: {file_path}")
                continue
            tokenized_text = tokenizer(text, max_length=block_size, truncation=True, padding='max_length', return_tensors="pt")
            self.input_ids.append(tokenized_text['input_ids'].squeeze(0))
            self.attention_masks.append(tokenized_text['attention_mask'].squeeze(0))
        if not self.input_ids:
            raise ValueError("No valid data was tokenized. Check your input files and tokenizer settings.")
        self.input_ids = torch.stack(self.input_ids)
        self.attention_masks = torch.stack(self.attention_masks)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_masks[idx], 'labels': self.input_ids[idx]}

folder_path = '/content/drive/My Drive/NSA Dataset'
file_paths = list_files_recursively(folder_path)
file_paths = [f for f in file_paths if f.endswith('.txt') or f.endswith('.html')]

if not file_paths:
    raise ValueError("No text or HTML files found in the specified folder. Check your folder path and file extensions.")

tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')
dataset = CodeDataset(tokenizer, file_paths)

model = AutoModelForMaskedLM.from_pretrained('microsoft/codebert-base')
training_args = TrainingArguments(
    output_dir='/content/drive/My Drive/Model Training Results',
    num_train_epochs=3,
    per_device_train_batch_size=2,
    logging_dir='./logs',
    save_steps=100
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

trainer.train()
model.save_pretrained('/content/drive/My Drive/Trained Models/my_codebert')

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForMaskedLM
import torch

app = FastAPI()

class TextRequest(BaseModel):
    text: str

class TextResponse(BaseModel):
    original_text: str
    generated_text: str

# Load tokenizer and model just once when the server starts
tokenizer = AutoTokenizer.from_pretrained('/content/drive/My Drive/Trained Models/my_codebert')
model = AutoModelForMaskedLM.from_pretrained('/content/drive/My Drive/Trained Models/my_codebert')

@app.post("/predict", response_model=TextResponse)
async def predict(request: TextRequest):
    inputs = tokenizer.encode(request.text, return_tensors="pt", truncation=True, max_length=512)
    outputs = model.generate(inputs, max_length=512)
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return TextResponse(
        original_text=request.text,
        generated_text=generated_text
    )

!ls "/content/drive/My Drive/Trained Models/my_codebert"
